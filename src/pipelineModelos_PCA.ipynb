{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries, checking their version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2\n",
      "IPython version: 6.2.1\n",
      "numpy version: 1.13.3\n",
      "pandas version: 0.21.0\n",
      "scikit-learn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "np.set_printoptions(threshold='nan')\n",
    "\n",
    "print ('Python version: %s.%s.%s' % sys.version_info[:3])\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('pandas version:', pd.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data were processed previously in R, and saved in _feather_ format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VisitNumber</th>\n",
       "      <th>TripType</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Upc</th>\n",
       "      <th>ScanCount</th>\n",
       "      <th>DepartmentDescription</th>\n",
       "      <th>FinelineNumber</th>\n",
       "      <th>DepartmentGroup</th>\n",
       "      <th>numItems</th>\n",
       "      <th>num_purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>friday</td>\n",
       "      <td>6414410235</td>\n",
       "      <td>11</td>\n",
       "      <td>dsd grocery</td>\n",
       "      <td>2008</td>\n",
       "      <td>food</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>friday</td>\n",
       "      <td>2800053970</td>\n",
       "      <td>11</td>\n",
       "      <td>candy, tobacco, cookies</td>\n",
       "      <td>115</td>\n",
       "      <td>food</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>friday</td>\n",
       "      <td>7794800902</td>\n",
       "      <td>11</td>\n",
       "      <td>dsd grocery</td>\n",
       "      <td>7950</td>\n",
       "      <td>food</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>friday</td>\n",
       "      <td>4383</td>\n",
       "      <td>11</td>\n",
       "      <td>produce</td>\n",
       "      <td>3102</td>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "      <td>friday</td>\n",
       "      <td>32878550911</td>\n",
       "      <td>11</td>\n",
       "      <td>infant consumable hardlines</td>\n",
       "      <td>2009</td>\n",
       "      <td>infant</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VisitNumber TripType Weekday          Upc  ScanCount  \\\n",
       "0          10        8  friday   6414410235         11   \n",
       "1          10        8  friday   2800053970         11   \n",
       "2          10        8  friday   7794800902         11   \n",
       "3         100       37  friday         4383         11   \n",
       "4        1000        9  friday  32878550911         11   \n",
       "\n",
       "         DepartmentDescription FinelineNumber DepartmentGroup  numItems  \\\n",
       "0                  dsd grocery           2008            food         3   \n",
       "1      candy, tobacco, cookies            115            food         3   \n",
       "2                  dsd grocery           7950            food         3   \n",
       "3                      produce           3102            food         1   \n",
       "4  infant consumable hardlines           2009          infant         1   \n",
       "\n",
       "   num_purchased  \n",
       "0             33  \n",
       "1             33  \n",
       "2             33  \n",
       "3             11  \n",
       "4             11  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leyendo datos creados con Rstudio\n",
    "import feather\n",
    "walmart = feather.read_dataframe('../data/transformed_data.feather')\n",
    "walmart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for _Weekday_ and _DepartmentGroup_ variables; _Upc, FinelineNumber, ScanCount_ and _DepartmentDescription_ were removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VisitNumber TripType  numItems  num_purchased  Weekday_friday  \\\n",
      "0          10        8         3             33               1   \n",
      "1          10        8         3             33               1   \n",
      "2          10        8         3             33               1   \n",
      "\n",
      "   Weekday_monday  Weekday_saturday  Weekday_sunday  Weekday_thursday  \\\n",
      "0               0                 0               0                 0   \n",
      "1               0                 0               0                 0   \n",
      "2               0                 0               0                 0   \n",
      "\n",
      "   Weekday_tuesday          ...            DepartmentGroup_cloth  \\\n",
      "0                0          ...                                0   \n",
      "1                0          ...                                0   \n",
      "2                0          ...                                0   \n",
      "\n",
      "   DepartmentGroup_media and gaming  DepartmentGroup_house  \\\n",
      "0                                 0                      0   \n",
      "1                                 0                      0   \n",
      "2                                 0                      0   \n",
      "\n",
      "   DepartmentGroup_girls wear, 4-6x  and 7-14  DepartmentGroup_home  \\\n",
      "0                                           0                     0   \n",
      "1                                           0                     0   \n",
      "2                                           0                     0   \n",
      "\n",
      "   DepartmentGroup_garden  DepartmentGroup_infant  DepartmentGroup_null  \\\n",
      "0                       0                       0                     0   \n",
      "1                       0                       0                     0   \n",
      "2                       0                       0                     0   \n",
      "\n",
      "   DepartmentGroup_office  DepartmentGroup_games  \n",
      "0                       0                      0  \n",
      "1                       0                      0  \n",
      "2                       0                      0  \n",
      "\n",
      "[3 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.get_dummies(walmart, columns =['Weekday', 'DepartmentGroup'])\n",
    "columns = ['Upc', 'FinelineNumber', 'ScanCount', 'DepartmentDescription']#, 'VisitNumber']\n",
    "data2.drop(columns, inplace=True, axis=1)\n",
    "print(data2.loc[data2['VisitNumber'] == '10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 26 features, two new dataframes are created in order to have a final dataframe, with one row per VisitNumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisitNumber    category\n",
      "TripType       category\n",
      "dtype: object\n",
      "  VisitNumber TripType\n",
      "0          10        8\n",
      "3         100       37\n",
      "4        1000        9\n",
      "5      100002        9\n",
      "6      100003      999\n",
      "(95674, 2)\n",
      "       VisitNumber TripType\n",
      "458078           5      999\n"
     ]
    }
   ],
   "source": [
    "temp = data2[['VisitNumber','TripType']]\n",
    "temp = temp.drop_duplicates(subset=['VisitNumber'], keep='first')\n",
    "\n",
    "print(temp.dtypes)\n",
    "print(temp.head(5))\n",
    "print(temp.shape)\n",
    "\n",
    "print(temp[temp['VisitNumber'] == '5'])\n",
    "#columns = ['TripType'] #,  'numItems',  'num_purchased' ]#, 'VisitNumber']\n",
    "#aux.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean operator to numerical variables, grouped by _VisitNumber_, in order to keep the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 25)\n",
      "  VisitNumber  numItems  num_purchased  Weekday_friday  Weekday_monday  \\\n",
      "0           5         1             10             1.0             0.0   \n",
      "\n",
      "   Weekday_saturday  Weekday_sunday  Weekday_thursday  Weekday_tuesday  \\\n",
      "0               0.0             0.0               0.0              0.0   \n",
      "\n",
      "   Weekday_wednesday          ...            DepartmentGroup_cloth  \\\n",
      "0                0.0          ...                              0.0   \n",
      "\n",
      "   DepartmentGroup_media and gaming  DepartmentGroup_house  \\\n",
      "0                               0.0                    0.0   \n",
      "\n",
      "   DepartmentGroup_girls wear, 4-6x  and 7-14  DepartmentGroup_home  \\\n",
      "0                                         0.0                   0.0   \n",
      "\n",
      "   DepartmentGroup_garden  DepartmentGroup_infant  DepartmentGroup_null  \\\n",
      "0                     0.0                     0.0                   0.0   \n",
      "\n",
      "   DepartmentGroup_office  DepartmentGroup_games  \n",
      "0                     0.0                    0.0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "aux = data2\n",
    "aux = aux.groupby([\"VisitNumber\"]).mean().reset_index()\n",
    "aux = aux.dropna(axis=0, how='any')\n",
    "print(aux.shape)\n",
    "#print(aux.columns.values)\n",
    "print(aux[aux['VisitNumber'] == '5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final data_frame has 95,674 observations, each one is a _VisitNumber_, with 26 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 25)\n",
      "(95674, 2)\n",
      "(95674, 26)\n"
     ]
    }
   ],
   "source": [
    "print(aux.shape)\n",
    "print(temp.shape)\n",
    "df = temp.join(aux.set_index('VisitNumber'), on='VisitNumber', how=\"right\")\n",
    "#df = aux.merge(temp, on=['VisitNumber'], how='left', indicator=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VisitNumber TripType  numItems  num_purchased  Weekday_friday  \\\n",
      "0          10        8         3             33             1.0   \n",
      "3         100       37         1             11             1.0   \n",
      "\n",
      "   Weekday_monday  Weekday_saturday  Weekday_sunday  Weekday_thursday  \\\n",
      "0             0.0               0.0             0.0               0.0   \n",
      "3             0.0               0.0             0.0               0.0   \n",
      "\n",
      "   Weekday_tuesday          ...            DepartmentGroup_cloth  \\\n",
      "0              0.0          ...                              0.0   \n",
      "3              0.0          ...                              0.0   \n",
      "\n",
      "   DepartmentGroup_media and gaming  DepartmentGroup_house  \\\n",
      "0                               0.0                    0.0   \n",
      "3                               0.0                    0.0   \n",
      "\n",
      "   DepartmentGroup_girls wear, 4-6x  and 7-14  DepartmentGroup_home  \\\n",
      "0                                         0.0                   0.0   \n",
      "3                                         0.0                   0.0   \n",
      "\n",
      "   DepartmentGroup_garden  DepartmentGroup_infant  DepartmentGroup_null  \\\n",
      "0                     0.0                     0.0                   0.0   \n",
      "3                     0.0                     0.0                   0.0   \n",
      "\n",
      "   DepartmentGroup_office  DepartmentGroup_games  \n",
      "0                     0.0                    0.0  \n",
      "3                     0.0                    0.0  \n",
      "\n",
      "[2 rows x 26 columns]\n",
      "VisitNumber                                   0\n",
      "TripType                                      0\n",
      "numItems                                      0\n",
      "num_purchased                                 0\n",
      "Weekday_friday                                0\n",
      "Weekday_monday                                0\n",
      "Weekday_saturday                              0\n",
      "Weekday_sunday                                0\n",
      "Weekday_thursday                              0\n",
      "Weekday_tuesday                               0\n",
      "Weekday_wednesday                             0\n",
      "DepartmentGroup_other departments             0\n",
      "DepartmentGroup_accessories                   0\n",
      "DepartmentGroup_electronics                   0\n",
      "DepartmentGroup_food                          0\n",
      "DepartmentGroup_health & beauty               0\n",
      "DepartmentGroup_cloth                         0\n",
      "DepartmentGroup_media and gaming              0\n",
      "DepartmentGroup_house                         0\n",
      "DepartmentGroup_girls wear, 4-6x  and 7-14    0\n",
      "DepartmentGroup_home                          0\n",
      "DepartmentGroup_garden                        0\n",
      "DepartmentGroup_infant                        0\n",
      "DepartmentGroup_null                          0\n",
      "DepartmentGroup_office                        0\n",
      "DepartmentGroup_games                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head(2))\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding categorical dependent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farid/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.iloc[:, df.columns != 'TripType']\n",
    "y = df.iloc[:, df.columns=='TripType'] \n",
    "\n",
    "label_encoder = LabelEncoder()  ## Para convertir a enteros\n",
    "\n",
    "## Convertirmos a enteros, etc\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA in order to reduce dimensionality of dataframe, making training a little less slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674, 10)\n"
     ]
    }
   ],
   "source": [
    "#Usando PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)\n",
    "\n",
    "X = pca.transform(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farid/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/farid/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tutorial](http://www.codiply.com/blog/hyperparameter-grid-search-across-multiple-models-in-scikit-learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First  dictionary:* models to be scored <br>\n",
    "*Second dictionary:* parameters for each model <br>\n",
    "*Fit:* returns a paremeter grid search with cross validation for each model and for the given data <br>\n",
    "*Score_summary:* returns a data_frame with a summary of the scores <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "class EstimatorSelectionHelper:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "    \n",
    "    def fit(self, X, y, cv=10, n_jobs=1, verbose=1, scoring=None, refit=True):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, \n",
    "                              params, \n",
    "                              cv=cv, \n",
    "                              n_jobs=n_jobs, \n",
    "                              verbose=verbose, \n",
    "                              scoring=scoring, \n",
    "                              refit=refit)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "    \n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "            #return pd.Series(dict(params.items() + d.items()))\n",
    "                      \n",
    "        rows = [row(k, gsc.cv_validation_scores, gsc.parameters) \n",
    "                for k in self.keys\n",
    "                for gsc in self.grid_searches[k].grid_scores_]\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        \n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        \n",
    "        return df[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints, just two machine learning models were tested... (**It is so SLOW!!!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (ExtraTreesClassifier, RandomForestClassifier, \n",
    "                              AdaBoostClassifier, GradientBoostingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models1 = { \n",
    "    #'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    #'RandomForestClassifier': RandomForestClassifier(),\n",
    "    #'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    #'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    #'LogisticRegression' : LogisticRegression(),\n",
    "    #'KNeighborsClassifier' : KNeighborsClassifier(),\n",
    "    #'NaiveBayes': MultinomialNB(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "params1 = { \n",
    "    #'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n",
    "    #'RandomForestClassifier': { 'n_estimators': [50,100], 'max_depth': [100, 200], 'max_features':['sqrt', 'log2'], 'min_samples_split': [10, 50] }#,\n",
    "    #'AdaBoostClassifier':  { 'n_estimators': [16, 32] },\n",
    "    #'GradientBoostingClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
    "    #'LogisticRegression' : { 'C' : [1, 1e3, 1e5] },\n",
    "    #'KNeighborsClassifier' : { 'n_neighbors' : [3,5] },\n",
    "    #'NaiveBayes' : { 'alpha' : [0.1, 0.001, 0.0001] },\n",
    "    'SVC': [\n",
    "        {'kernel': ['linear'], 'C': [1, 10, 100]},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10, 100], 'gamma': [0.001, 0.0001]},\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for SVC.\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "%time helper1.fit(X_train, y_train, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing performance of models, with different parameters (on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper1.score_summary(sort_by='min_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some graphs about the best model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924456812867\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=100, max_features='sqrt', min_samples_split=5, n_estimators=50) #, gamma=0.001)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "y_pred = model.predict(X_test)\n",
    "#print (confusion_matrix(y_test, y_pred))\n",
    "#print (classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farid/.pyenv/versions/3.5.2/lib/python3.5/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95674,)\n"
     ]
    }
   ],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = df.iloc[:, df.columns != 'TripType']\n",
    "y = df.iloc[:, df.columns=='TripType'] \n",
    "\n",
    "label_encoder = LabelEncoder()  ## Para convertir a enteros\n",
    "\n",
    "## Convertirmos a enteros, etc\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y.shape)\n",
    "\n",
    "label_encoder = LabelEncoder()  ## Para convertir a enteros\n",
    "#one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "## Convertirmos a enteros, i.e. setosa -> 0, etc\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=range(41))\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=0)\n",
    "\n",
    "classifier = OneVsRestClassifier(SVC(C=10, kernel='linear', probability=True))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "print(y_test[0:10], y_score[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ROC curve and ROC area for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = label_binarize(y, classes=[0, 1, 2]).shape[1]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = n_classes\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Vs Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "\n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    #plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no for multiclass labels\n",
    "\n",
    "plot_precision_recall_n(y_test[:,0], y_score[:,0], classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                        y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "    y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro-average precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.3\n",
      "IPython version: 6.2.1\n",
      "numpy version: 1.13.3\n",
      "pandas version: 0.21.0\n",
      "scikit-learn version: 0.19.1\n",
      "  VisitNumber  Weekday  TripType  numItems  Return  CategoryCounts  \\\n",
      "0           5        5       999         1     0.0               1   \n",
      "1           7        5        30         4     0.0               1   \n",
      "2           8        5        26       529     0.0               1   \n",
      "3           9        5         8         9     0.0               1   \n",
      "4          10        5         8         9     0.0               1   \n",
      "\n",
      "   frozen foods  pharmacy  seafood  electronics     ...      \\\n",
      "0             0         0        0            0     ...       \n",
      "1             0         0        0            0     ...       \n",
      "2             0         0        0            0     ...       \n",
      "3             0         0        0            0     ...       \n",
      "4             0         0        0            0     ...       \n",
      "\n",
      "   financial services  sheer hosiery  paint and accessories  automotive  \\\n",
      "0                   1              0                      0           0   \n",
      "1                   0              0                      0           0   \n",
      "2                   0              0                     16           0   \n",
      "3                   0              0                      0           0   \n",
      "4                   0              0                      0           0   \n",
      "\n",
      "   liquor,wine,beer  jewelry and sunglasses  home decor  dsd grocery  \\\n",
      "0                 0                       0           0            0   \n",
      "1                 0                       0           0            0   \n",
      "2                 0                       0           0            1   \n",
      "3                 0                       0           0            0   \n",
      "4                 0                       0           0            2   \n",
      "\n",
      "   sporting goods  TripType_l  \n",
      "0               0          37  \n",
      "1               0          22  \n",
      "2               0          18  \n",
      "3               0           5  \n",
      "4               0           5  \n",
      "\n",
      "[5 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sys\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "print ('Python version: %s.%s.%s' % sys.version_info[:3])\n",
    "print ('IPython version:', IPython.__version__)\n",
    "print ('numpy version:', np.__version__)\n",
    "print ('pandas version:', pd.__version__)\n",
    "print ('scikit-learn version:', sk.__version__)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "import feather\n",
    "## Usando OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#data = pd.read_csv(\"../data/train.csv\")\n",
    "data = feather.read_dataframe('../data/transformed_data.feather')\n",
    "data['Weekday'] = data['Weekday'].map({\"monday\": 1, \"tuesday\": 2, \"wednesday\": 3, \"thursday\": 4, \"friday\": 5,\n",
    "\"saturday\": 6, \"sunday\": 7})\n",
    "for each in ['TripType', 'Weekday']:\n",
    "    data[each] = data[each].astype(int)\n",
    "\n",
    "\n",
    "def add_category_counts(data):\n",
    "    alist = []\n",
    "    for array in np.asarray(data.iloc[:,11:]):\n",
    "        count = 0\n",
    "        for item in array:\n",
    "            if item > 0:\n",
    "                count += 1\n",
    "        alist.append(count)\n",
    "    cat_counts = pd.DataFrame(alist)\n",
    "    cat_counts = cat_counts.rename(columns={0:\"CategoryCount\"})\n",
    "    cat_counts = cat_counts.set_index(data.index)\n",
    "    data.insert(11, 'CategoryCounts', cat_counts)\n",
    "    return data\n",
    "\n",
    "dummies = pd.get_dummies(data.DepartmentDescription)\n",
    "data[dummies.columns] = dummies\n",
    "data_dummies = data[dummies.columns] \n",
    "data_dummies = data_dummies.apply(lambda x: x*data[\"ScanCount\"])\n",
    "data.loc[data.ScanCount < 0, 'Return'] = 1\n",
    "data.loc[data.Return != 1, 'Return'] = 0\n",
    "add_category_counts(data)\n",
    "grouped = data.groupby(\"VisitNumber\")\n",
    "elset = set(data.DepartmentDescription)\n",
    "aggregate = {'Weekday': np.max, \"TripType\": np.max, 'numItems': np.sum, 'Return': np.max, 'CategoryCounts': np.max}\n",
    "for each in list(elset):\n",
    "    if each != np.nan:\n",
    "        aggregate[each] = np.sum\n",
    "data = grouped.agg(aggregate).reset_index()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder() \n",
    " \n",
    "data['TripType_l'] = label_encoder.fit_transform(data['TripType'])\n",
    "\n",
    "mytrain, mytest = train_test_split(data, test_size = .7)\n",
    "features = ['VisitNumber', 'Weekday', 'Return', 'CategoryCounts'] + list(elset)\n",
    "print(data.head())\n",
    "train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.63329\teval-mlogloss:2.65786\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 3 rounds.\n",
      "[1]\ttrain-mlogloss:2.21439\teval-mlogloss:2.25639\n",
      "[2]\ttrain-mlogloss:1.98769\teval-mlogloss:2.04151\n",
      "[3]\ttrain-mlogloss:1.82774\teval-mlogloss:1.89187\n",
      "[4]\ttrain-mlogloss:1.70782\teval-mlogloss:1.78095\n",
      "[5]\ttrain-mlogloss:1.61352\teval-mlogloss:1.69477\n",
      "[6]\ttrain-mlogloss:1.53689\teval-mlogloss:1.62586\n",
      "[7]\ttrain-mlogloss:1.47406\teval-mlogloss:1.5703\n",
      "[8]\ttrain-mlogloss:1.42065\teval-mlogloss:1.52408\n",
      "[9]\ttrain-mlogloss:1.37574\teval-mlogloss:1.48581\n",
      "[10]\ttrain-mlogloss:1.33716\teval-mlogloss:1.45379\n",
      "[11]\ttrain-mlogloss:1.30335\teval-mlogloss:1.42622\n",
      "[12]\ttrain-mlogloss:1.27324\teval-mlogloss:1.40242\n",
      "[13]\ttrain-mlogloss:1.24721\teval-mlogloss:1.38197\n",
      "[14]\ttrain-mlogloss:1.22314\teval-mlogloss:1.36402\n",
      "[15]\ttrain-mlogloss:1.20193\teval-mlogloss:1.348\n",
      "[16]\ttrain-mlogloss:1.18288\teval-mlogloss:1.33461\n",
      "[17]\ttrain-mlogloss:1.16565\teval-mlogloss:1.32243\n",
      "[18]\ttrain-mlogloss:1.14922\teval-mlogloss:1.31125\n",
      "[19]\ttrain-mlogloss:1.13429\teval-mlogloss:1.30097\n",
      "[20]\ttrain-mlogloss:1.12055\teval-mlogloss:1.29183\n",
      "[21]\ttrain-mlogloss:1.10799\teval-mlogloss:1.28404\n",
      "[22]\ttrain-mlogloss:1.09684\teval-mlogloss:1.27747\n",
      "[23]\ttrain-mlogloss:1.08636\teval-mlogloss:1.27066\n",
      "[24]\ttrain-mlogloss:1.07665\teval-mlogloss:1.26535\n",
      "[25]\ttrain-mlogloss:1.06669\teval-mlogloss:1.25976\n",
      "[26]\ttrain-mlogloss:1.05731\teval-mlogloss:1.25439\n",
      "[27]\ttrain-mlogloss:1.04876\teval-mlogloss:1.24966\n",
      "[28]\ttrain-mlogloss:1.04117\teval-mlogloss:1.24543\n",
      "[29]\ttrain-mlogloss:1.03362\teval-mlogloss:1.24149\n",
      "[30]\ttrain-mlogloss:1.02638\teval-mlogloss:1.23778\n",
      "[31]\ttrain-mlogloss:1.01982\teval-mlogloss:1.23452\n",
      "[32]\ttrain-mlogloss:1.0128\teval-mlogloss:1.23131\n",
      "[33]\ttrain-mlogloss:1.00637\teval-mlogloss:1.22819\n",
      "[34]\ttrain-mlogloss:1.00031\teval-mlogloss:1.22549\n",
      "[35]\ttrain-mlogloss:0.99428\teval-mlogloss:1.22285\n",
      "[36]\ttrain-mlogloss:0.988334\teval-mlogloss:1.22029\n",
      "[37]\ttrain-mlogloss:0.982583\teval-mlogloss:1.21805\n",
      "[38]\ttrain-mlogloss:0.976793\teval-mlogloss:1.21551\n",
      "[39]\ttrain-mlogloss:0.971845\teval-mlogloss:1.21345\n",
      "[40]\ttrain-mlogloss:0.96682\teval-mlogloss:1.21169\n",
      "[41]\ttrain-mlogloss:0.962328\teval-mlogloss:1.20984\n",
      "[42]\ttrain-mlogloss:0.957792\teval-mlogloss:1.20828\n",
      "[43]\ttrain-mlogloss:0.953083\teval-mlogloss:1.20678\n",
      "[44]\ttrain-mlogloss:0.948174\teval-mlogloss:1.20511\n",
      "[45]\ttrain-mlogloss:0.943255\teval-mlogloss:1.20346\n",
      "[46]\ttrain-mlogloss:0.938671\teval-mlogloss:1.20218\n",
      "[47]\ttrain-mlogloss:0.934059\teval-mlogloss:1.2007\n",
      "[48]\ttrain-mlogloss:0.929722\teval-mlogloss:1.19946\n",
      "[49]\ttrain-mlogloss:0.925527\teval-mlogloss:1.19832\n",
      "[50]\ttrain-mlogloss:0.921577\teval-mlogloss:1.19712\n",
      "[51]\ttrain-mlogloss:0.917393\teval-mlogloss:1.19558\n",
      "[52]\ttrain-mlogloss:0.913939\teval-mlogloss:1.19434\n",
      "[53]\ttrain-mlogloss:0.909998\teval-mlogloss:1.19335\n",
      "[54]\ttrain-mlogloss:0.906369\teval-mlogloss:1.19247\n",
      "[55]\ttrain-mlogloss:0.902289\teval-mlogloss:1.19128\n",
      "[56]\ttrain-mlogloss:0.898265\teval-mlogloss:1.19002\n",
      "[57]\ttrain-mlogloss:0.894759\teval-mlogloss:1.18921\n",
      "[58]\ttrain-mlogloss:0.891298\teval-mlogloss:1.1884\n",
      "[59]\ttrain-mlogloss:0.888059\teval-mlogloss:1.18736\n",
      "[60]\ttrain-mlogloss:0.884462\teval-mlogloss:1.18665\n",
      "[61]\ttrain-mlogloss:0.881485\teval-mlogloss:1.18586\n",
      "[62]\ttrain-mlogloss:0.878308\teval-mlogloss:1.18495\n",
      "[63]\ttrain-mlogloss:0.875104\teval-mlogloss:1.18414\n",
      "[64]\ttrain-mlogloss:0.871788\teval-mlogloss:1.18348\n",
      "[65]\ttrain-mlogloss:0.868657\teval-mlogloss:1.18285\n",
      "[66]\ttrain-mlogloss:0.865829\teval-mlogloss:1.1822\n",
      "[67]\ttrain-mlogloss:0.862932\teval-mlogloss:1.18165\n",
      "[68]\ttrain-mlogloss:0.860209\teval-mlogloss:1.18102\n",
      "[69]\ttrain-mlogloss:0.857233\teval-mlogloss:1.18033\n",
      "[70]\ttrain-mlogloss:0.854116\teval-mlogloss:1.17973\n",
      "[71]\ttrain-mlogloss:0.851088\teval-mlogloss:1.17906\n",
      "[72]\ttrain-mlogloss:0.847859\teval-mlogloss:1.17854\n",
      "[73]\ttrain-mlogloss:0.845127\teval-mlogloss:1.17787\n",
      "[74]\ttrain-mlogloss:0.842598\teval-mlogloss:1.17771\n",
      "[75]\ttrain-mlogloss:0.839867\teval-mlogloss:1.17719\n",
      "[76]\ttrain-mlogloss:0.836967\teval-mlogloss:1.17671\n",
      "[77]\ttrain-mlogloss:0.834805\teval-mlogloss:1.17626\n",
      "[78]\ttrain-mlogloss:0.832215\teval-mlogloss:1.17583\n",
      "[79]\ttrain-mlogloss:0.829141\teval-mlogloss:1.17533\n",
      "[80]\ttrain-mlogloss:0.826552\teval-mlogloss:1.17487\n",
      "[81]\ttrain-mlogloss:0.824265\teval-mlogloss:1.17434\n",
      "[82]\ttrain-mlogloss:0.821937\teval-mlogloss:1.17434\n",
      "[83]\ttrain-mlogloss:0.819851\teval-mlogloss:1.17395\n",
      "[84]\ttrain-mlogloss:0.81726\teval-mlogloss:1.17363\n",
      "[85]\ttrain-mlogloss:0.815309\teval-mlogloss:1.17343\n",
      "[86]\ttrain-mlogloss:0.812896\teval-mlogloss:1.17304\n",
      "[87]\ttrain-mlogloss:0.810501\teval-mlogloss:1.17298\n",
      "[88]\ttrain-mlogloss:0.807759\teval-mlogloss:1.17263\n",
      "[89]\ttrain-mlogloss:0.805148\teval-mlogloss:1.1725\n",
      "[90]\ttrain-mlogloss:0.802762\teval-mlogloss:1.17236\n",
      "[91]\ttrain-mlogloss:0.800543\teval-mlogloss:1.17211\n",
      "[92]\ttrain-mlogloss:0.798254\teval-mlogloss:1.17224\n",
      "[93]\ttrain-mlogloss:0.796232\teval-mlogloss:1.17191\n",
      "[94]\ttrain-mlogloss:0.794014\teval-mlogloss:1.17205\n",
      "[95]\ttrain-mlogloss:0.791866\teval-mlogloss:1.17188\n",
      "[96]\ttrain-mlogloss:0.789462\teval-mlogloss:1.17169\n",
      "[97]\ttrain-mlogloss:0.787318\teval-mlogloss:1.17146\n",
      "[98]\ttrain-mlogloss:0.785446\teval-mlogloss:1.17145\n",
      "[99]\ttrain-mlogloss:0.783491\teval-mlogloss:1.17139\n",
      "[100]\ttrain-mlogloss:0.781376\teval-mlogloss:1.17123\n",
      "[101]\ttrain-mlogloss:0.779458\teval-mlogloss:1.1711\n",
      "[102]\ttrain-mlogloss:0.77733\teval-mlogloss:1.17083\n",
      "[103]\ttrain-mlogloss:0.775554\teval-mlogloss:1.17074\n",
      "[104]\ttrain-mlogloss:0.773216\teval-mlogloss:1.17097\n",
      "[105]\ttrain-mlogloss:0.771138\teval-mlogloss:1.17085\n",
      "[106]\ttrain-mlogloss:0.769143\teval-mlogloss:1.17083\n",
      "Stopping. Best iteration:\n",
      "[103]\ttrain-mlogloss:0.775554\teval-mlogloss:1.17074\n",
      "\n",
      "CPU times: user 14min 56s, sys: 8.29 s, total: 15min 5s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "mytrain, mytest = train_test_split(data, test_size = .3)\n",
    "\n",
    "dtrain = xgb.DMatrix(np.asarray(mytrain[features]), label = np.asarray(mytrain.TripType_l))\n",
    "dtest = xgb.DMatrix(np.asarray(mytest[features]), label = np.asarray(mytest.TripType_l))\n",
    "num_round = 150\n",
    "param = {'objective': 'multi:softprob', 'num_class':len(set(mytrain.TripType_l)), \n",
    "     'eval_metric': 'mlogloss', \"max_delta_step\": 5}\n",
    "watchlist = [(dtrain,'train'), (dtest, 'eval')]\n",
    "\n",
    "%time bst = xgb.train(param, dtrain, num_round, watchlist,early_stopping_rounds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58854475141971219"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predictions(predictions):\n",
    "    return [np.argmax(each) for each in predictions]\n",
    "        \n",
    "predictions = get_predictions(bst.predict(dtest))\n",
    "\n",
    "#predictions = label_encoder.inverse_transform(predictions)\n",
    "accuracy_score(mytest.TripType_l, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = feather.read_dataframe('../data/test_transformed_data.feather')\n",
    "data['Weekday'] = data['Weekday'].map({\"monday\": 1, \"tuesday\": 2, \"wednesday\": 3, \"thursday\": 4, \"friday\": 5,\n",
    "\"saturday\": 6, \"sunday\": 7})\n",
    "for each in ['Weekday']:\n",
    "    data[each] = data[each].astype(int)\n",
    "\n",
    "\n",
    "dummies = pd.get_dummies(data.DepartmentDescription)\n",
    "data[dummies.columns] = dummies\n",
    "data_dummies = data[dummies.columns] \n",
    "data_dummies = data_dummies.apply(lambda x: x*data[\"ScanCount\"])\n",
    "\n",
    "data.loc[data.ScanCount < 0, 'Return'] = 1\n",
    "data.loc[data.Return != 1, 'Return'] = 0\n",
    "\n",
    "\n",
    "grouped = data.groupby(\"VisitNumber\")\n",
    "elset = set(data.DepartmentDescription)\n",
    "aggregate = {'Weekday': np.max, 'numItems': np.sum, 'Return': np.max}\n",
    "for each in list(elset):\n",
    "    if each != np.nan:\n",
    "        aggregate[each] = np.sum\n",
    "data = grouped.agg(aggregate).reset_index()\n",
    "add_category_counts(data)\n",
    "\n",
    "data['health & beauty'] = np.zeros(len(data))\n",
    "\n",
    "test = data\n",
    "data_test = xgb.DMatrix(np.asarray(test[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 267 ms, total: 1min 2s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%time predictions = get_predictions(bst.predict(data_test))\n",
    "predictions = label_encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95674"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['TripType'] = predictions\n",
    "answers = test[['TripType', 'VisitNumber']]\n",
    "answers = pd.get_dummies(answers, columns =['TripType'])\n",
    "answers['TripType_14'] = np.zeros(len(answers))\n",
    "answers['TripType_4'] = np.zeros(len(answers))\n",
    "answers.to_csv('../data/final_submission.csv', index=False)\n",
    "len(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
